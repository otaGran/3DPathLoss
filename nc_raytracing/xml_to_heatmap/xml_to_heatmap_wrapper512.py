import subprocess
import concurrent
from concurrent.futures import wait, as_completed
import os
import time
import multiprocessing
from PIL import Image
#import tensorflow as tf
import numpy as np
from tqdm import tqdm
# Import Sionna RT components
#from sionna.rt import load_scene, Transmitter, Receiver, PlanarArray, Camera, Paths2CIR


EXTRA_HEIGHT = 2  # height of Rx above ground. 
CM_RESOLUTION = 4  # size in meters of each pixel in the coverage map
ANTENNA_PATTERN = 'iso'  # 'tr38901' also possible


# testing new Blender_command_line function written on 23. Jun 2023
BASE_PATH_BLENDER = '/home/yl826/res_plane/'
#BASE_PATH_SIONNA = '/dev/shm/coverage_maps_data_aug_Jul20_1000/'
CM_NUM_SAMPLES = int(7e6)  # input for the num_samples in scene.coverage_map (sionna method), default 2e6
BASE_PATH_SIONNA = '/home/yl826/3DPathLoss/nc_raytracing/cm_512_Aug10_' + str(int(CM_NUM_SAMPLES / (1e6))) + \
                    'e6_' + ANTENNA_PATTERN + 'Tx/'

os.makedirs(BASE_PATH_SIONNA, exist_ok=True)
# BASE_PATH_SIONNA = '/home/yl826/3DPathLoss/nc_raytracing/Sionna_coverage_maps/coverage_maps_plane_missing_Jul6/'
# un-comment 
# BASE_PATH_BLENDER = 'res/res_23Jun23/'
# BASE_PATH_SIONNA = 'Sionna_coverage_maps/coverage_maps_new_22Jun23/'
# START_FROM_IDX = 512
STOP_AT_IDX = int(1e7)  # larger than 7000 to be safe when running all building maps. 


NUM_OF_PROCESS = 12
NUMBER_OF_GPU = 2
    

def initializer_func(gpu_seq_queue: multiprocessing.Queue, log_level: int) -> None:
    """
    This is a initializer function run after the creation of each process in ProcessPoolExecutor, 
    to set the os env variable to limit the visiablity of GPU for each process inorder to achieve 
    the load balance bewteen diff GPU
    :gpu_seq_queue This is a queue storing the GPU ID as a token, each process will only get 
    """
    import os
    
    gpu_id = gpu_seq_queue.get()
    print("Initlizing the process: %d with GPU: %d"%(os.getpid(),gpu_id))
    
    # Configure visible GPU 
    os.environ["CUDA_VISIBLE_DEVICES"] = f"{gpu_id}"
    
    
#     import tensorflow as tf
    
#     gpus = tf.config.list_physical_devices('GPU')
#     print(gpus)
#     if gpus:
#         try:
#             for gpu in gpus:
#                 tf.config.experimental.set_memory_growth(gpu, True)
#             logical_gpus = tf.config.list_logical_devices('GPU')
#             print(len(gpus), 'Physical GPUs, ', len(logical_gpus), 'Logical GPUs')
#         except RuntimeError as e:
#             print(e)  # Avoid warnings from TensorFlow
    
if __name__ == '__main__':
    # this gets the idx_uuid
    f_names_xml = [f for f in os.listdir(BASE_PATH_BLENDER + 'Bl_xml_files/')
                   if os.path.isdir(BASE_PATH_BLENDER + 'Bl_xml_files/' + f)]
    print('Number of xml files:', len(f_names_xml))
    
    
    # f[0:-4] to remove the ".npy" from file name, 
    # but we now have _xCoord_yCoord_heightVal append after the idx_uuid, 
    # so we do this instead:
    f_names_sig_map_idx_uuid = list(set([f.split('_')[0] + '_' + f.split('_')[1] 
                                         for f in os.listdir(BASE_PATH_SIONNA)
                                         if os.path.isfile(BASE_PATH_SIONNA + f)]))
    #print('Number of finished signal maps:', len(f_names_sig_map_idx_uuid))
    futures = []
    #print(f_names_sig_map_idx_uuid)
    
    # Create a GPU ID token Queue
    gpu_seq_queue = multiprocessing.Queue()

    for i in range(NUM_OF_PROCESS):
        gpu_seq_queue.put(i%NUMBER_OF_GPU)
    
    # Init pbar
    pbar = tqdm(total=len(f_names_xml), desc='xml_to_heatmap512')
    count = 0
    # Init process pool executor
    with concurrent.futures.ProcessPoolExecutor(
        max_workers=NUM_OF_PROCESS, initializer=initializer_func, initargs=(gpu_seq_queue,1)) as executor:
        for idx, f_name_xml in enumerate(f_names_xml):
            
            if idx > STOP_AT_IDX: 
                break
            
            if f_name_xml not in f_names_sig_map_idx_uuid:  
                # skip cmaps that have already been generated by considering 
                # only the idx_uuid part of generated maps' names. Avoids unnecessary 
                # and costly subprocesses for coverage maps that have already been computed. 
                print("yes")
                """
                Creating a subprocess for each job running in process pool
                This is the simples way I can find to free up the GPU memory 
                and do a load balance betwenn each GPU
                """
                futures.append(executor.submit(subprocess.run,
                       ['python', 'xml_to_heatmap_one_run512.py',
                        '--file_name_wo_type', str(f_name_xml),
                        '--extra_height', str(EXTRA_HEIGHT),
                        '--cm_cell_size', str(CM_RESOLUTION),
                        '--BASE_PATH_BLENDER', str(BASE_PATH_BLENDER),
                        '--BASE_PATH_SIONNA', str(BASE_PATH_SIONNA),
                        '--outer_idx', str(idx), 
                        '--cm_num_samples', str(CM_NUM_SAMPLES), 
                        '--antenna_pattern', str(ANTENNA_PATTERN)],
                         capture_output=True, text=True))

            
                print(' '.join(
                    ['python', 'xml_to_heatmap_one_run512.py',
                    '--file_name_wo_type', str(f_name_xml),
                    '--extra_height', str(EXTRA_HEIGHT),
                    '--cm_cell_size', str(CM_RESOLUTION),
                    '--BASE_PATH_BLENDER', str(BASE_PATH_BLENDER).replace(' ', '\ '),
                    '--BASE_PATH_SIONNA', str(BASE_PATH_SIONNA).replace(' ', '\ '),
                    '--outer_idx', str(idx), 
                    '--cm_num_samples', str(CM_NUM_SAMPLES), 
                    '--antenna_pattern', str(ANTENNA_PATTERN)]
                ))
    
        for idx, future in enumerate(as_completed(futures)):
            pbar.update(n=1)  
            try:
                data = str(future.result()).replace('\\n','\n')
                print('\n\n\n\n\n' + str(idx) + '\n' + data + '\n\n\n\n\n')
            except Exception as err:
                print(err)
    print('DONE')
    
